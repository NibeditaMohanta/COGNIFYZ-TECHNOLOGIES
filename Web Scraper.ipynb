{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64388dd7-03f3-4771-89bb-e5fcc124daee",
   "metadata": {},
   "source": [
    "### Level 3 Task 1: Build a Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec96d8d5-1f9b-48d0-b6b1-fb2898325340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Powerful web scraper\n",
      "\t\t\t\t\t\tfor regular and\n",
      "\t\t\t\t\t\tprofessional use\n",
      "2. Extract data from the most complex websites\n",
      "3. Point-and-click interface\n",
      "4. Extract data from dynamic web sites\n",
      "5. Handle JavaScript sites\n",
      "6. Use sitemaps to customize data\n",
      "7. Export data in CSV, XLSX and JSON formats\n",
      "8. Start using Web Scraper now!\n",
      "9. Streamline your data collection\n",
      "10. Automate data extraction in the cloud\n",
      "11. Integrate data with any system\n",
      "12. 99.9% success rate\n",
      "13. Get started in 4 steps\n",
      "14. STEP 1\n",
      "15. STEP 2\n",
      "16. STEP 3\n",
      "17. STEP 4\n",
      "18. Done!\n",
      "19. Pricing\n",
      "20. Browser extension\n",
      "21. Project\n",
      "22. Professional\n",
      "23. Business\n",
      "24. Scale\n",
      "25. Benefit from industry leading support\n",
      "26. Diego Kremer\n",
      "27. Carlos Figueroa\n",
      "28. Jonathan H\n",
      "29. About Web Scraper\n",
      "30. Founded in 2017\n",
      "31. Located in Latvia, EU\n",
      "32. Serving over 800'000+ users across the world\n",
      "33. Obsessed with great customer support\n",
      "34. Frequently asked questions\n",
      "35. What is a URL Credit?\n",
      "36. Will I be able to scrape a specific site?\n",
      "37. Do I need to input my credit card information to start free trial?\n",
      "38. How Scale plan differs from other plans?\n",
      "39. Can I upgrade or downgrade my subscription plan?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_titles(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    titles = []\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3']):\n",
    "        titles.append(heading.get_text(strip=True))\n",
    "    return titles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://webscraper.io/\"  # Replace with your target website\n",
    "    titles = scrape_titles(url)\n",
    "    for idx, title in enumerate(titles, 1):\n",
    "        print(f\"{idx}. {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464a0a6-75b5-4a94-a3ef-61fa03672047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
